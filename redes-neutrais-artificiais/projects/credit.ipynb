{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade nbformat -q\n",
    "!pip install plotly -q\n",
    "!pip install pandas -q\n",
    "!pip install numpy -q\n",
    "!pip install seaborn -q\n",
    "!pip install matplotlib -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install yellowbrick -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base de cr√©dito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../assets/credit.pkl\", \"rb\") as file:\n",
    "    X_credit_train, y_credit_train, X_credit_test, y_credit_test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_train.shape, y_credit_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (1500,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_test.shape, y_credit_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.83160188\n",
      "Iteration 2, loss = 0.69313619\n",
      "Iteration 3, loss = 0.57673267\n",
      "Iteration 4, loss = 0.48116809\n",
      "Iteration 5, loss = 0.40418229\n",
      "Iteration 6, loss = 0.34242153\n",
      "Iteration 7, loss = 0.29162211\n",
      "Iteration 8, loss = 0.24952315\n",
      "Iteration 9, loss = 0.21634609\n",
      "Iteration 10, loss = 0.19087041\n",
      "Iteration 11, loss = 0.17031552\n",
      "Iteration 12, loss = 0.15390421\n",
      "Iteration 13, loss = 0.14056674\n",
      "Iteration 14, loss = 0.12970751\n",
      "Iteration 15, loss = 0.12104107\n",
      "Iteration 16, loss = 0.11427422\n",
      "Iteration 17, loss = 0.10741003\n",
      "Iteration 18, loss = 0.10219432\n",
      "Iteration 19, loss = 0.09771443\n",
      "Iteration 20, loss = 0.09330080\n",
      "Iteration 21, loss = 0.08938344\n",
      "Iteration 22, loss = 0.08579740\n",
      "Iteration 23, loss = 0.08283699\n",
      "Iteration 24, loss = 0.07967546\n",
      "Iteration 25, loss = 0.07690714\n",
      "Iteration 26, loss = 0.07421034\n",
      "Iteration 27, loss = 0.07171612\n",
      "Iteration 28, loss = 0.06963389\n",
      "Iteration 29, loss = 0.06716494\n",
      "Iteration 30, loss = 0.06495397\n",
      "Iteration 31, loss = 0.06330999\n",
      "Iteration 32, loss = 0.06083247\n",
      "Iteration 33, loss = 0.05906331\n",
      "Iteration 34, loss = 0.05730146\n",
      "Iteration 35, loss = 0.05548314\n",
      "Iteration 36, loss = 0.05362921\n",
      "Iteration 37, loss = 0.05212028\n",
      "Iteration 38, loss = 0.05049178\n",
      "Iteration 39, loss = 0.04909855\n",
      "Iteration 40, loss = 0.04758722\n",
      "Iteration 41, loss = 0.04615154\n",
      "Iteration 42, loss = 0.04476172\n",
      "Iteration 43, loss = 0.04355678\n",
      "Iteration 44, loss = 0.04224866\n",
      "Iteration 45, loss = 0.04135895\n",
      "Iteration 46, loss = 0.04015561\n",
      "Iteration 47, loss = 0.03912943\n",
      "Iteration 48, loss = 0.03792064\n",
      "Iteration 49, loss = 0.03718787\n",
      "Iteration 50, loss = 0.03619834\n",
      "Iteration 51, loss = 0.03542347\n",
      "Iteration 52, loss = 0.03486478\n",
      "Iteration 53, loss = 0.03382548\n",
      "Iteration 54, loss = 0.03313916\n",
      "Iteration 55, loss = 0.03248518\n",
      "Iteration 56, loss = 0.03192200\n",
      "Iteration 57, loss = 0.03114376\n",
      "Iteration 58, loss = 0.03070606\n",
      "Iteration 59, loss = 0.03013801\n",
      "Iteration 60, loss = 0.02958376\n",
      "Iteration 61, loss = 0.02890953\n",
      "Iteration 62, loss = 0.02838908\n",
      "Iteration 63, loss = 0.02807481\n",
      "Iteration 64, loss = 0.02735005\n",
      "Iteration 65, loss = 0.02692349\n",
      "Iteration 66, loss = 0.02650415\n",
      "Iteration 67, loss = 0.02604356\n",
      "Iteration 68, loss = 0.02566214\n",
      "Iteration 69, loss = 0.02556657\n",
      "Iteration 70, loss = 0.02512824\n",
      "Iteration 71, loss = 0.02466077\n",
      "Iteration 72, loss = 0.02439807\n",
      "Iteration 73, loss = 0.02368425\n",
      "Iteration 74, loss = 0.02345431\n",
      "Iteration 75, loss = 0.02319684\n",
      "Iteration 76, loss = 0.02283561\n",
      "Iteration 77, loss = 0.02236304\n",
      "Iteration 78, loss = 0.02227978\n",
      "Iteration 79, loss = 0.02193770\n",
      "Iteration 80, loss = 0.02129084\n",
      "Iteration 81, loss = 0.02166545\n",
      "Iteration 82, loss = 0.02063224\n",
      "Iteration 83, loss = 0.02076757\n",
      "Iteration 84, loss = 0.02027901\n",
      "Iteration 85, loss = 0.01993095\n",
      "Iteration 86, loss = 0.01958199\n",
      "Iteration 87, loss = 0.01954989\n",
      "Iteration 88, loss = 0.01915087\n",
      "Iteration 89, loss = 0.01895657\n",
      "Iteration 90, loss = 0.01881791\n",
      "Iteration 91, loss = 0.01852409\n",
      "Iteration 92, loss = 0.01841323\n",
      "Iteration 93, loss = 0.01827114\n",
      "Iteration 94, loss = 0.01797835\n",
      "Iteration 95, loss = 0.01769137\n",
      "Iteration 96, loss = 0.01741103\n",
      "Iteration 97, loss = 0.01725406\n",
      "Iteration 98, loss = 0.01714713\n",
      "Iteration 99, loss = 0.01675890\n",
      "Iteration 100, loss = 0.01693970\n",
      "Iteration 101, loss = 0.01666458\n",
      "Iteration 102, loss = 0.01624013\n",
      "Iteration 103, loss = 0.01613321\n",
      "Iteration 104, loss = 0.01598645\n",
      "Iteration 105, loss = 0.01559247\n",
      "Iteration 106, loss = 0.01550695\n",
      "Iteration 107, loss = 0.01538324\n",
      "Iteration 108, loss = 0.01514098\n",
      "Iteration 109, loss = 0.01508630\n",
      "Iteration 110, loss = 0.01525809\n",
      "Iteration 111, loss = 0.01507189\n",
      "Iteration 112, loss = 0.01468764\n",
      "Iteration 113, loss = 0.01452166\n",
      "Iteration 114, loss = 0.01425605\n",
      "Iteration 115, loss = 0.01410276\n",
      "Iteration 116, loss = 0.01408354\n",
      "Iteration 117, loss = 0.01383340\n",
      "Iteration 118, loss = 0.01372453\n",
      "Iteration 119, loss = 0.01368599\n",
      "Iteration 120, loss = 0.01342491\n",
      "Iteration 121, loss = 0.01323799\n",
      "Iteration 122, loss = 0.01327056\n",
      "Iteration 123, loss = 0.01309582\n",
      "Iteration 124, loss = 0.01303402\n",
      "Iteration 125, loss = 0.01279410\n",
      "Iteration 126, loss = 0.01297918\n",
      "Iteration 127, loss = 0.01248703\n",
      "Iteration 128, loss = 0.01240423\n",
      "Iteration 129, loss = 0.01229498\n",
      "Iteration 130, loss = 0.01211458\n",
      "Iteration 131, loss = 0.01209805\n",
      "Iteration 132, loss = 0.01191767\n",
      "Iteration 133, loss = 0.01180537\n",
      "Iteration 134, loss = 0.01178150\n",
      "Iteration 135, loss = 0.01183472\n",
      "Iteration 136, loss = 0.01165354\n",
      "Iteration 137, loss = 0.01151885\n",
      "Iteration 138, loss = 0.01191988\n",
      "Iteration 139, loss = 0.01115535\n",
      "Iteration 140, loss = 0.01115016\n",
      "Iteration 141, loss = 0.01124083\n",
      "Iteration 142, loss = 0.01156921\n",
      "Iteration 143, loss = 0.01077674\n",
      "Iteration 144, loss = 0.01111431\n",
      "Iteration 145, loss = 0.01059355\n",
      "Iteration 146, loss = 0.01057198\n",
      "Iteration 147, loss = 0.01052069\n",
      "Iteration 148, loss = 0.01033095\n",
      "Iteration 149, loss = 0.01030701\n",
      "Iteration 150, loss = 0.01025292\n",
      "Iteration 151, loss = 0.01033739\n",
      "Iteration 152, loss = 0.01049238\n",
      "Iteration 153, loss = 0.01030110\n",
      "Iteration 154, loss = 0.00990720\n",
      "Iteration 155, loss = 0.00986667\n",
      "Iteration 156, loss = 0.00972597\n",
      "Iteration 157, loss = 0.00969508\n",
      "Iteration 158, loss = 0.00955561\n",
      "Iteration 159, loss = 0.00979915\n",
      "Iteration 160, loss = 0.00946033\n",
      "Iteration 161, loss = 0.00949784\n",
      "Iteration 162, loss = 0.00935618\n",
      "Iteration 163, loss = 0.00930023\n",
      "Iteration 164, loss = 0.00911166\n",
      "Iteration 165, loss = 0.00899650\n",
      "Iteration 166, loss = 0.00900355\n",
      "Iteration 167, loss = 0.00885308\n",
      "Iteration 168, loss = 0.00882669\n",
      "Iteration 169, loss = 0.00880932\n",
      "Iteration 170, loss = 0.00893108\n",
      "Iteration 171, loss = 0.00890290\n",
      "Iteration 172, loss = 0.00855661\n",
      "Iteration 173, loss = 0.00890666\n",
      "Iteration 174, loss = 0.00868126\n",
      "Iteration 175, loss = 0.00860504\n",
      "Iteration 176, loss = 0.00830901\n",
      "Iteration 177, loss = 0.00833892\n",
      "Iteration 178, loss = 0.00814355\n",
      "Iteration 179, loss = 0.00816389\n",
      "Iteration 180, loss = 0.00799233\n",
      "Iteration 181, loss = 0.00792572\n",
      "Iteration 182, loss = 0.00798874\n",
      "Iteration 183, loss = 0.00783608\n",
      "Iteration 184, loss = 0.00788549\n",
      "Iteration 185, loss = 0.00791407\n",
      "Iteration 186, loss = 0.00774719\n",
      "Iteration 187, loss = 0.00768110\n",
      "Iteration 188, loss = 0.00768582\n",
      "Iteration 189, loss = 0.00738491\n",
      "Iteration 190, loss = 0.00742345\n",
      "Iteration 191, loss = 0.00725705\n",
      "Iteration 192, loss = 0.00759155\n",
      "Iteration 193, loss = 0.00724392\n",
      "Iteration 194, loss = 0.00720371\n",
      "Iteration 195, loss = 0.00708482\n",
      "Iteration 196, loss = 0.00721207\n",
      "Iteration 197, loss = 0.00697711\n",
      "Iteration 198, loss = 0.00709691\n",
      "Iteration 199, loss = 0.00710738\n",
      "Iteration 200, loss = 0.00713613\n",
      "Iteration 201, loss = 0.00686321\n",
      "Iteration 202, loss = 0.00677931\n",
      "Iteration 203, loss = 0.00704749\n",
      "Iteration 204, loss = 0.00698307\n",
      "Iteration 205, loss = 0.00664634\n",
      "Iteration 206, loss = 0.00678539\n",
      "Iteration 207, loss = 0.00663923\n",
      "Iteration 208, loss = 0.00652553\n",
      "Iteration 209, loss = 0.00659754\n",
      "Iteration 210, loss = 0.00642044\n",
      "Iteration 211, loss = 0.00636965\n",
      "Iteration 212, loss = 0.00636779\n",
      "Iteration 213, loss = 0.00624188\n",
      "Iteration 214, loss = 0.00628661\n",
      "Iteration 215, loss = 0.00618406\n",
      "Iteration 216, loss = 0.00619148\n",
      "Iteration 217, loss = 0.00626815\n",
      "Iteration 218, loss = 0.00608458\n",
      "Iteration 219, loss = 0.00612198\n",
      "Iteration 220, loss = 0.00604888\n",
      "Iteration 221, loss = 0.00595642\n",
      "Iteration 222, loss = 0.00616612\n",
      "Iteration 223, loss = 0.00577906\n",
      "Iteration 224, loss = 0.00601724\n",
      "Iteration 225, loss = 0.00602028\n",
      "Iteration 226, loss = 0.00603521\n",
      "Iteration 227, loss = 0.00571510\n",
      "Iteration 228, loss = 0.00569075\n",
      "Iteration 229, loss = 0.00581205\n",
      "Iteration 230, loss = 0.00563060\n",
      "Iteration 231, loss = 0.00556058\n",
      "Iteration 232, loss = 0.00553825\n",
      "Iteration 233, loss = 0.00550827\n",
      "Iteration 234, loss = 0.00545731\n",
      "Iteration 235, loss = 0.00553245\n",
      "Iteration 236, loss = 0.00539889\n",
      "Iteration 237, loss = 0.00537834\n",
      "Iteration 238, loss = 0.00588796\n",
      "Iteration 239, loss = 0.00558051\n",
      "Iteration 240, loss = 0.00540670\n",
      "Iteration 241, loss = 0.00554339\n",
      "Iteration 242, loss = 0.00519926\n",
      "Iteration 243, loss = 0.00531726\n",
      "Iteration 244, loss = 0.00513072\n",
      "Iteration 245, loss = 0.00551042\n",
      "Iteration 246, loss = 0.00563253\n",
      "Iteration 247, loss = 0.00549352\n",
      "Iteration 248, loss = 0.00535720\n",
      "Iteration 249, loss = 0.00532737\n",
      "Iteration 250, loss = 0.00500436\n",
      "Iteration 251, loss = 0.00510634\n",
      "Iteration 252, loss = 0.00492553\n",
      "Iteration 253, loss = 0.00486876\n",
      "Iteration 254, loss = 0.00475940\n",
      "Iteration 255, loss = 0.00489364\n",
      "Iteration 256, loss = 0.00478153\n",
      "Iteration 257, loss = 0.00470281\n",
      "Iteration 258, loss = 0.00477716\n",
      "Iteration 259, loss = 0.00470508\n",
      "Iteration 260, loss = 0.00468232\n",
      "Iteration 261, loss = 0.00459002\n",
      "Iteration 262, loss = 0.00461328\n",
      "Iteration 263, loss = 0.00482979\n",
      "Iteration 264, loss = 0.00450079\n",
      "Iteration 265, loss = 0.00455219\n",
      "Iteration 266, loss = 0.00442651\n",
      "Iteration 267, loss = 0.00440434\n",
      "Iteration 268, loss = 0.00446150\n",
      "Iteration 269, loss = 0.00441756\n",
      "Iteration 270, loss = 0.00429668\n",
      "Iteration 271, loss = 0.00448563\n",
      "Iteration 272, loss = 0.00428782\n",
      "Iteration 273, loss = 0.00431418\n",
      "Iteration 274, loss = 0.00430899\n",
      "Iteration 275, loss = 0.00418011\n",
      "Iteration 276, loss = 0.00431515\n",
      "Iteration 277, loss = 0.00442906\n",
      "Iteration 278, loss = 0.00419450\n",
      "Iteration 279, loss = 0.00464888\n",
      "Iteration 280, loss = 0.00410154\n",
      "Iteration 281, loss = 0.00412316\n",
      "Iteration 282, loss = 0.00401579\n",
      "Iteration 283, loss = 0.00399723\n",
      "Iteration 284, loss = 0.00404644\n",
      "Iteration 285, loss = 0.00402342\n",
      "Iteration 286, loss = 0.00456244\n",
      "Iteration 287, loss = 0.00459565\n",
      "Iteration 288, loss = 0.00411225\n",
      "Iteration 289, loss = 0.00429352\n",
      "Iteration 290, loss = 0.00456994\n",
      "Iteration 291, loss = 0.00371596\n",
      "Iteration 292, loss = 0.00425269\n",
      "Iteration 293, loss = 0.00437580\n",
      "Iteration 294, loss = 0.00416522\n",
      "Iteration 295, loss = 0.00374925\n",
      "Iteration 296, loss = 0.00371786\n",
      "Iteration 297, loss = 0.00365813\n",
      "Iteration 298, loss = 0.00374315\n",
      "Iteration 299, loss = 0.00370071\n",
      "Iteration 300, loss = 0.00388491\n",
      "Iteration 301, loss = 0.00420122\n",
      "Iteration 302, loss = 0.00362609\n",
      "Iteration 303, loss = 0.00368879\n",
      "Iteration 304, loss = 0.00351872\n",
      "Iteration 305, loss = 0.00355091\n",
      "Iteration 306, loss = 0.00352030\n",
      "Iteration 307, loss = 0.00347313\n",
      "Iteration 308, loss = 0.00360960\n",
      "Iteration 309, loss = 0.00343675\n",
      "Iteration 310, loss = 0.00342354\n",
      "Iteration 311, loss = 0.00337572\n",
      "Iteration 312, loss = 0.00352658\n",
      "Iteration 313, loss = 0.00351795\n",
      "Iteration 314, loss = 0.00338996\n",
      "Iteration 315, loss = 0.00342903\n",
      "Iteration 316, loss = 0.00332771\n",
      "Iteration 317, loss = 0.00334174\n",
      "Iteration 318, loss = 0.00329692\n",
      "Iteration 319, loss = 0.00344970\n",
      "Iteration 320, loss = 0.00326734\n",
      "Iteration 321, loss = 0.00335550\n",
      "Iteration 322, loss = 0.00319978\n",
      "Iteration 323, loss = 0.00327989\n",
      "Iteration 324, loss = 0.00319585\n",
      "Iteration 325, loss = 0.00333798\n",
      "Iteration 326, loss = 0.00327635\n",
      "Iteration 327, loss = 0.00313922\n",
      "Iteration 328, loss = 0.00315737\n",
      "Iteration 329, loss = 0.00309731\n",
      "Iteration 330, loss = 0.00315994\n",
      "Iteration 331, loss = 0.00301787\n",
      "Iteration 332, loss = 0.00308041\n",
      "Iteration 333, loss = 0.00298745\n",
      "Iteration 334, loss = 0.00297242\n",
      "Iteration 335, loss = 0.00300108\n",
      "Iteration 336, loss = 0.00300387\n",
      "Iteration 337, loss = 0.00292615\n",
      "Iteration 338, loss = 0.00303664\n",
      "Iteration 339, loss = 0.00291051\n",
      "Iteration 340, loss = 0.00302271\n",
      "Iteration 341, loss = 0.00307434\n",
      "Iteration 342, loss = 0.00283471\n",
      "Iteration 343, loss = 0.00289328\n",
      "Iteration 344, loss = 0.00286381\n",
      "Iteration 345, loss = 0.00287327\n",
      "Iteration 346, loss = 0.00279878\n",
      "Iteration 347, loss = 0.00287160\n",
      "Iteration 348, loss = 0.00279047\n",
      "Iteration 349, loss = 0.00275653\n",
      "Iteration 350, loss = 0.00282095\n",
      "Iteration 351, loss = 0.00279945\n",
      "Iteration 352, loss = 0.00277142\n",
      "Iteration 353, loss = 0.00268347\n",
      "Iteration 354, loss = 0.00276302\n",
      "Iteration 355, loss = 0.00270212\n",
      "Iteration 356, loss = 0.00264548\n",
      "Iteration 357, loss = 0.00267495\n",
      "Iteration 358, loss = 0.00268678\n",
      "Iteration 359, loss = 0.00257178\n",
      "Iteration 360, loss = 0.00258944\n",
      "Iteration 361, loss = 0.00263643\n",
      "Iteration 362, loss = 0.00253310\n",
      "Iteration 363, loss = 0.00258246\n",
      "Iteration 364, loss = 0.00250903\n",
      "Iteration 365, loss = 0.00257668\n",
      "Iteration 366, loss = 0.00251382\n",
      "Iteration 367, loss = 0.00254148\n",
      "Iteration 368, loss = 0.00258374\n",
      "Iteration 369, loss = 0.00253080\n",
      "Iteration 370, loss = 0.00242170\n",
      "Iteration 371, loss = 0.00249195\n",
      "Iteration 372, loss = 0.00247155\n",
      "Iteration 373, loss = 0.00246424\n",
      "Iteration 374, loss = 0.00269536\n",
      "Iteration 375, loss = 0.00296494\n",
      "Iteration 376, loss = 0.00255085\n",
      "Iteration 377, loss = 0.00257624\n",
      "Iteration 378, loss = 0.00245530\n",
      "Iteration 379, loss = 0.00235602\n",
      "Iteration 380, loss = 0.00250302\n",
      "Iteration 381, loss = 0.00269236\n",
      "Iteration 382, loss = 0.00237963\n",
      "Iteration 383, loss = 0.00240628\n",
      "Iteration 384, loss = 0.00234696\n",
      "Iteration 385, loss = 0.00228376\n",
      "Iteration 386, loss = 0.00225439\n",
      "Iteration 387, loss = 0.00243188\n",
      "Iteration 388, loss = 0.00220235\n",
      "Iteration 389, loss = 0.00234836\n",
      "Iteration 390, loss = 0.00219108\n",
      "Iteration 391, loss = 0.00226078\n",
      "Iteration 392, loss = 0.00227900\n",
      "Iteration 393, loss = 0.00245708\n",
      "Iteration 394, loss = 0.00218833\n",
      "Iteration 395, loss = 0.00223729\n",
      "Iteration 396, loss = 0.00225166\n",
      "Iteration 397, loss = 0.00217331\n",
      "Iteration 398, loss = 0.00213225\n",
      "Iteration 399, loss = 0.00214693\n",
      "Iteration 400, loss = 0.00228453\n",
      "Iteration 401, loss = 0.00237996\n",
      "Iteration 402, loss = 0.00204507\n",
      "Iteration 403, loss = 0.00212594\n",
      "Iteration 404, loss = 0.00210797\n",
      "Iteration 405, loss = 0.00221488\n",
      "Iteration 406, loss = 0.00207720\n",
      "Iteration 407, loss = 0.00205836\n",
      "Iteration 408, loss = 0.00201506\n",
      "Iteration 409, loss = 0.00204138\n",
      "Iteration 410, loss = 0.00200133\n",
      "Iteration 411, loss = 0.00207917\n",
      "Iteration 412, loss = 0.00188995\n",
      "Iteration 413, loss = 0.00211131\n",
      "Iteration 414, loss = 0.00198515\n",
      "Iteration 415, loss = 0.00200231\n",
      "Iteration 416, loss = 0.00197074\n",
      "Iteration 417, loss = 0.00193289\n",
      "Iteration 418, loss = 0.00209149\n",
      "Iteration 419, loss = 0.00212629\n",
      "Iteration 420, loss = 0.00197591\n",
      "Iteration 421, loss = 0.00199015\n",
      "Iteration 422, loss = 0.00193500\n",
      "Iteration 423, loss = 0.00183738\n",
      "Iteration 424, loss = 0.00189371\n",
      "Iteration 425, loss = 0.00186442\n",
      "Iteration 426, loss = 0.00183336\n",
      "Iteration 427, loss = 0.00188786\n",
      "Iteration 428, loss = 0.00179545\n",
      "Iteration 429, loss = 0.00176928\n",
      "Iteration 430, loss = 0.00181303\n",
      "Iteration 431, loss = 0.00204953\n",
      "Iteration 432, loss = 0.00189642\n",
      "Iteration 433, loss = 0.00220517\n",
      "Iteration 434, loss = 0.00186900\n",
      "Iteration 435, loss = 0.00176215\n",
      "Iteration 436, loss = 0.00176824\n",
      "Iteration 437, loss = 0.00175712\n",
      "Iteration 438, loss = 0.00175652\n",
      "Iteration 439, loss = 0.00170257\n",
      "Iteration 440, loss = 0.00172742\n",
      "Iteration 441, loss = 0.00167110\n",
      "Iteration 442, loss = 0.00167066\n",
      "Iteration 443, loss = 0.00173593\n",
      "Iteration 444, loss = 0.00172344\n",
      "Iteration 445, loss = 0.00170946\n",
      "Iteration 446, loss = 0.00160103\n",
      "Iteration 447, loss = 0.00172920\n",
      "Iteration 448, loss = 0.00173602\n",
      "Iteration 449, loss = 0.00168346\n",
      "Iteration 450, loss = 0.00185979\n",
      "Iteration 451, loss = 0.00167961\n",
      "Iteration 452, loss = 0.00162783\n",
      "Iteration 453, loss = 0.00162894\n",
      "Iteration 454, loss = 0.00167743\n",
      "Iteration 455, loss = 0.00170507\n",
      "Iteration 456, loss = 0.00159882\n",
      "Iteration 457, loss = 0.00160125\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_credit = MLPClassifier(\n",
    "    max_iter=1500, verbose=True, tol=0.0000100, solver=\"adam\", activation=\"relu\", hidden_layer_sizes=(50, 50)\n",
    ")\n",
    "neural_network_credit.fit(X_credit_train, y_credit_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = neural_network_credit.predict(X_credit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_credit_test, predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(neural_network_credit)\n",
    "cm.fit(X_credit_train, y_credit_train)\n",
    "cm.score(X_credit_test, y_credit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      0.97      0.98        64\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.99      0.98      0.99       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_test, predicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
